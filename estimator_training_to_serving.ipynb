{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a Resnet TF Estimator Model\n",
    "\n",
    "**Scenario:** In Tensorflow 1.3, a higher level API called Estimators was introduced and has since been a popular API of choice within the Tensorflow community. Suppose that an ML researcher has trained a Resnet model on the Imagenet dataset using Tensorflow's Estimator API, located at https://github.com/tensorflow/models/tree/v1.4.0/official/resnet. (Note that we used v1.4.0. You always want to use a stable tag for a model version to deploy as the researcher can continue to modify the model and architecture at the head of master.) Our task is to deploy this model into Tensorflow Serving. You have access to their python code as well as a saved state (checkpoint) that points to their favorite trained result.\n",
    "\n",
    "The first step is to create a servable version of the model that will be used for Tensorflow Serving, which runs very efficiently in C++, and is platform independent (can run on different OSes, as well as hardware with different types of accelerators such as GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant indicating the number of layers in our loaded model. We're loading a resnet-50 model.\n",
    "RESNET_SIZE = 50  \n",
    "\n",
    "# Model and serving directories\n",
    "MODEL_DIR=\"resnet_model_checkpoints\"\n",
    "SERVING_DIR=\"resnet_servable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "\n",
    "The next step is to load the researcher's saved checkpoint into our estimator. We will download it from\n",
    "http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz using the commands below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz \", \"resnet.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the file into a directory called resnet\n",
    "from subprocess import call\n",
    "call([\"mkdir\", MODEL_DIR])\n",
    "call([\"tar\", \"-zxvf\", \"resnet.tar.gz\", \"-C\", MODEL_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you see model checkpoint files in this directory\n",
    "os.listdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Model Architecture\n",
    " \n",
    "In order to reconstruct the Resnet neural network used to train the Imagenet model, we need to load the architecture pieces. During the setup step, we checked out https://github.com/tensorflow/models/tree/v1.4.0/official/resnet into the parent directory + \"/models\". We can now load functions and constants from resnet_model.py into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/official/resnet/resnet_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** We also need to import some constants from [imagenet_main.py](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py), but we cannot run this file as it is a main class that will attempt to train ResNet. Open [imagenet_main.py](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py) and copy over a few constants that are important--namely, the image size, channels, and number of classes--into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy constants from imagenet_main.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Servable from the Estimator API\n",
    "\n",
    "The [Tensorflow Estimator API](https://www.tensorflow.org/programmers_guide/estimators) is an abstraction that simplifies the process of training, evaluation, prediction, and serving. Central to the Estimator API is an argument called the model function (model_fn). Essentially, a model function defines which graph nodes are used in training, evaluation, and prediction. Depending on the mode (TRAIN, EVAL, PREDICT) used, the model function will return an [EstimatorSpec](https://www.tensorflow.org/versions/r1.3/extend/estimators#constructing_the_model_fn) object tell the Estimator to run different graph nodes. The typical behavior of a model function would be:\n",
    "\n",
    "* TRAIN mode calls an optimizer that is hooked to a loss function (e.g. cross-entropy), which depends on the logits for each class, which depends on lower layers of the network, etc.\n",
    "* EVAL mode does not call the optimizer, but calls the loss function and/or some other evaluation metric (e.g. accuracy). These evaluation metrics will likely depend on labels as well as the logits of the network output, which depends on lower layers of the network, etc.\n",
    "  * Additionally, researchers will often use monitors and hooks during training and evaluation to check on the progress of the model. Usually, these components are used to return summaries about different layers of the network, such as model coefficients, etc., which can be visualized using [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard).\n",
    "* PREDICT mode does NOT require an optimizer as there is no training step. Also, input data is unlabeled, so no loss functions or evaluation metrics are used. Instead, predictions simply try to provide clients/users with information of interest, such as the most likely label for an image, the probability of the image being of a particular class, etc.\n",
    "\n",
    "**Exercise:** Below is the training code used in the imagenet_main.py [resnet_model_fn()](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py#L162), renamed to serving_model_fn(). Portions of the code are modified and refactored into separate helper functions for debugging purposes. Since model serving is essentially prediction, graph elements associated with TRAIN and EVAL modes are no longer relevant. Remove/shortcut graph elements that are unrelated to prediction in the code cell below (marked with TODOs).\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.estimator.EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_model_fn(features, labels, mode):\n",
    "    '''The main model function used by the estimator to define the Tensorflow model server API.\n",
    "\n",
    "    Args:\n",
    "      features: The client request, which is a dictionary: {'image': 1D tensor of jpeg strings}\n",
    "      labels: None or not used since we are predicting only\n",
    "      mode: TRAIN, EVAL, or PREDICT. Serving only uses PREDICT mode.\n",
    "\n",
    "    Returns:\n",
    "      If training or evaluating (should not happen), return a blank EstimatorSpec that does nothing.\n",
    "      If predicting (always), return an EstimatorSpec that produces a response with top k classes\n",
    "        and probabilities to send back to the client.\n",
    "    '''\n",
    "\n",
    "    # TODO: Remove tf.summary.image(). This is used for monitoring during training.\n",
    "    tf.summary.image('images', features, max_outputs=6)\n",
    "\n",
    "    # Refactor preprocessing, network, and postprocessing into a serving_input_to_output() function.\n",
    "    predictions = serving_input_to_output(features, mode)\n",
    "\n",
    "    # Create the PREDICT EstimatorSpec that will send a proper response back to the client.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return create_servable_estimator_spec(predictions, mode)\n",
    "\n",
    "    # TODO: You already returned the EstimatorSpec for predictions. Training and evaluation are not needed.\n",
    "    # Shortcut every graph element below here by returning a minimal EstimatorSpec.\n",
    "    \n",
    "    # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "        logits=logits, onehot_labels=labels)\n",
    "\n",
    "    # Create a tensor named cross_entropy for logging purposes.\n",
    "    tf.identity(cross_entropy, name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    # Add weight decay to the loss. We exclude the batch norm variables because\n",
    "    # doing so leads to a small improvement in accuracy.\n",
    "    loss = cross_entropy + _WEIGHT_DECAY * tf.add_n(\n",
    "        [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "         if 'batch_normalization' not in v.name])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # Scale the learning rate linearly with the batch size. When the batch size\n",
    "        # is 256, the learning rate should be 0.1.\n",
    "        initial_learning_rate = 0.1 * params['batch_size'] / 256\n",
    "        batches_per_epoch = _NUM_IMAGES['train'] / params['batch_size']\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "        # Multiply the learning rate by 0.1 at 30, 60, 80, and 90 epochs.\n",
    "        boundaries = [\n",
    "            int(batches_per_epoch * epoch) for epoch in [30, 60, 80, 90]]\n",
    "        values = [\n",
    "            initial_learning_rate * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n",
    "        learning_rate = tf.train.piecewise_constant(\n",
    "            tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "        # Create a tensor named learning_rate for logging purposes.\n",
    "        tf.identity(learning_rate, name='learning_rate')\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=_MOMENTUM)\n",
    "\n",
    "        # Batch norm requires update_ops to be added as a train_op dependency.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "          train_op = optimizer.minimize(loss, global_step)\n",
    "    else:\n",
    "        train_op = None\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        tf.argmax(labels, axis=1), predictions['classes'])\n",
    "    metrics = {'accuracy': accuracy}\n",
    "\n",
    "    # Create a tensor named train_accuracy for logging purposes.\n",
    "    tf.identity(accuracy[1], name='train_accuracy')\n",
    "    tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "As discussed above, the code has been refactored into helper functions. The point of creating helper functions is two-fold:\n",
    "\n",
    "1. Modularity: you can reuse functions in different places; for instance, a different image model or ResNet architecture can reuse functions.\n",
    "2. Testability: you can unit test different parts of your code easily!\n",
    "\n",
    "We are going to focus on building simple helper functions and performing unit tests below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function: convert JPEG strings to Normalized 3D Tensors\n",
    "\n",
    "The [ResNet client](./client/resnet_client.py) sends jpeg encoded images into an array of jpegs (each entry a string) to send to the server. These jpegs are all appropriately resized to 224x224x3, and do not need resizing on the server side to enter into the ResNet model. However, the ResNet50 model was trained with pixel values normalized (approximately) between -0.5 and 0.5. We will need to extract the raw 3D tensor from each jpeg string and normalize the values.\n",
    "\n",
    "**Exercise:** Create a helper function that decodes a jpeg image, and normalizes pixel values to be between -0.5 and 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_jpeg_to_image(encoded_image):\n",
    "  \"\"\"Preprocesses the image by subtracting out the mean from all channels.\n",
    "  Args:\n",
    "    image: A jpeg-formatted byte stream represented as a string.\n",
    "  Returns:\n",
    "    A 3d tensor of image pixels normalized to be between -0.5 and 0.5, resized to height x width x 3.\n",
    "    The normalization is an approximation of the preprocess_for_train and preprocess_for_eval functions in\n",
    "    https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/vgg_preprocessing.py.\n",
    "  \"\"\"\n",
    "  image = ???  # TODO: Use a tf function to decode the jpeg into a 3d tensor.\n",
    "  image = tf.to_float(image) / 255.0 - 0.5  # Normalize values to be between -0.5 and 0.5.\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Helper function\n",
    "\n",
    "Using the code below, we can verify that the helper function works as expected. \n",
    "\n",
    "There are two parts to this process: constructing the Tensorflow graph, and running data through the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Test Graph\n",
    "\n",
    "During graph construction, Tensorflow will store a copy of your graph nodes in its own global dictionary. Input nodes for receiving data are called [placeholders](https://www.tensorflow.org/api_docs/python/tf/placeholder). A placeholder can store a Tensor of arbitrary dimension, and arbitrary length in any dimension. Once a placeholder is populated with input data, dependent nodes in your graph can then operate on the data and ultimately return an output. \n",
    "\n",
    "Tensorflow maintains a pointer to each graph component that is created. Thus, graph construction need only occur once! Even if you lose the Python reference to your graph node, e.g by running:\n",
    "\n",
    "```\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[10], 'my_input_node')\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[10], 'new_input_node')\n",
    "```\n",
    "\n",
    "the placeholder 'my_input_node' would still exist in the graph! (You can run print(x) to see how Tensorflow actually names your node inside its global dictionary.) The graph may still run without problems assuming you are referencing `x`, but you would have a duplicate unused placeholder in there.\n",
    "\n",
    "Likewise, we need a python variable to point to the output node that we want to collect data from. In this case, it is the return value of the convert_jpeg_to_image() function.\n",
    "\n",
    "**Caveat: convert_jpeg_to_image() does not return a 3D Tensor. It returns a graph node that returns a 3D Tensor after processing a jpeg-encoded string!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input test graph nodes: only needs to be run once!\n",
    "test_jpeg = tf.placeholder(dtype=tf.string, shape=[], name='test_jpeg')  # A placeholder for a single string, which is a dimensionless (0D) tensor.\n",
    "test_decoded_tensor = convert_jpeg_to_image(test_jpeg)  # Output node, which returns a 3D tensor after processing.\n",
    "\n",
    "# Print the graph elements to check shapes. ? indicates that Tensorflow does not know the length of those dimensions.\n",
    "print(test_jpeg)\n",
    "print(test_decoded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Test Graph\n",
    "\n",
    "Now we come to the data processing portion. To run data through a constructed Tensorflow graph, a session must be created to read and process input data. Tensorflow will only run a portion of the graph that is required to map a set of inputs (a dictionary of graph nodes, usually placeholders, as keys, and the input data as values) to an output graph node. This is invoked by the command:\n",
    "\n",
    "```\n",
    "tf.Session().run(output_graph_node,\n",
    "                 {input_graph_node_1: input_data_1, input_graph_node_2: input_data_2, ...})\n",
    "```\n",
    "\n",
    "To test the helper function, the input parameter encoded_image is a string, and the output is a 3d tensor. Since a string is dimensionless (or a 0-D tensor, its shape is empty []). The output of the function is a 3d tensor, which when returned by tf.Session().run(), becomes a 3D numpy array.\n",
    "\n",
    "**Exercise:** Add more potentially useful assert statements to test the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the result of the function using a sample image client/cat_sample.jpg\n",
    "\n",
    "with open(\"client/cat_sample.jpg\", \"rb\") as imageFile:\n",
    "    jpeg_str = imageFile.read()\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(test_decoded_tensor, feed_dict={test_jpeg: jpeg_str})\n",
    "        assert result.shape == (224, 224, 3)\n",
    "        # TODO: Replace with assert statements to check max and min normalized pixel values\n",
    "        assert False\n",
    "        print('Hooray! JPEG decoding test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "The approach above uses vanilla Tensorflow to perform unit testing. You may notice that the code is more verbose than ideal, since you have to create a session, feed input through a dictionary, etc. We encourage the student to try out some of the options below: \n",
    "\n",
    "[Tensorflow Eager](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html) was introduced in Tensorflow 1.5 as a way to execute Tensorflow graphs in a way similar to numpy operations. After testing individual parts of the graph using Eager, you will need to rebuild a graph with the Eager option turned off in order to build a performance optimized Tensorflow graph. Also, keep in mind that you will need another virtual environment with tensorflow 1.5 in order to run eager execution, which may not be compatible with Tensorflow Serving 1.4 used in this tutorial.\n",
    "\n",
    "[Tensorflow unit testing](https://www.tensorflow.org/api_guides/python/test) is a more software engineer oriented approach to run tests. By writing test classes that can be invoked individually when building the project, calling tf.test.main() will run all tests and return a list of ones that succeeded and failed, allowing you to inspect errors. Because we are in a notebook environment, such a test would not succeed due to an already running kernel that tf.test cannot access. The tests must be run from the command line, e.g. `python test_my_graph.py`.\n",
    "\n",
    "We've provided both eager execution and unit test examples in the [testing](./testing) directory showing how to unit test various components in this notebook. Note that because these examples contain the solution to exercises below, please complete all notebook exercises prior to reading through these examples.\n",
    "\n",
    "Now that we know how to run Tensorflow tests, let's create and test more helper functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Preprocessing Server Input\n",
    "\n",
    "The first component of the end-to-end graph is the preprocess_input() function. \n",
    "\n",
    "**Exercise**: We are going to change the way input features are read to be compliant with our [client](https://github.com/google-aai/tf-serving-k8s-tutorial/blob/master/client/resnet_client.py). As is often the case, training data comes in the form of a [TF Dataset](https://www.tensorflow.org/programmers_guide/datasets) with information such as labels, text, encoding, bounding boxes, etc. Our client's behavior is very simple: we want to send a message with a single field 'images' containing a batch (array) of jpeg-encoded images as strings. Follow the TODOs in the code below to complete the preprocess_input() helper function.\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.map_fn](https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "* [tf.DType](https://www.tensorflow.org/api_docs/python/tf/DType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(features):\n",
    "    '''Function to preprocess client request before feeding into the network.\n",
    "    \n",
    "    Use tf.map_fn and the convert_jpeg_to_image() helper function to convert the\n",
    "    1D input tensor of jpeg strings into a list of single-precision floating\n",
    "    point 3D tensors, which are normalized pixel values for the images.\n",
    "    \n",
    "    Then stack and reshape this list of tensors into a 4D tensor with\n",
    "    appropriate dimensions.\n",
    "    \n",
    "    Args:\n",
    "      features: request received from our client,\n",
    "        a dictionary with a single element containing a tensor of multiple jpeg images\n",
    "        {'images' : 1D_tensor_of_jpeg_byte_strings}\n",
    "    \n",
    "    Returns:\n",
    "      a 4D tensor of normalized pixel values for the input images.\n",
    "      \n",
    "    '''\n",
    "    images = features['images']  # A tensor of tf.strings\n",
    "    processed_images = ???  # TODO: fill in the ???\n",
    "    processed_images = tf.stack(processed_images)  # Convert list of 3D tensors to a 4D tensor\n",
    "    processed_images = tf.reshape(tensor=processed_images,  # Reshaping informs Tensorflow of the final dimensions of the 4D tensor\n",
    "                                  shape=[-1, _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, 3])\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test the Preprocessing Helper Function\n",
    "\n",
    "**Exercise**: Recall that your client is sending a message of the format:\n",
    "\n",
    "```\n",
    "{'images': array_of_strings}\n",
    "```\n",
    "\n",
    "The array_of_strings can be arbitrary length, and requires an entrypoint through a [placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) that can read in an arbitrary length array of strings. Fix the shape parameter to allow for an arbitrary length string array as input.\n",
    "\n",
    "**Hint:** You need to define the `shape` parameter in tf.placeholder. `None` inside an array indicates that the length can vary along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Test Input Preprocessing Network: only needs to be run once!\n",
    "test_jpeg_tensor = tf.placeholder(dtype=tf.string, shape=???, name='test_jpeg_tensor')  # A placeholder for a single string, which is a dimensionless (0D) tensor.\n",
    "test_processed_images = preprocess_input({'images': test_jpeg_tensor})  # Output node, which returns a 3D tensor after processing.\n",
    "\n",
    "# Print the graph elements to check shapes. ? indicates that Tensorflow does not know the length of those dimensions.\n",
    "print(test_jpeg_tensor)\n",
    "print(test_processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test network using a sample image client/cat_sample.jpg\n",
    "\n",
    "with open(\"client/cat_sample.jpg\", \"rb\") as imageFile:\n",
    "    jpeg_str = imageFile.read()\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(test_processed_images, feed_dict={test_jpeg_tensor: np.array([jpeg_str, jpeg_str])})  # Duplicate for length 2 array\n",
    "        assert result.shape == (2, 224, 224, 3)  # 4D tensor with first dimension length 2, since we have 2 images\n",
    "        # TODO: add a test for min and max normalized pixel values\n",
    "        assert False\n",
    "        # TODO: add a test to verify that the resulting tensor for image 0 and image 1 are identical.\n",
    "        assert False\n",
    "        print('Hooray! Input unit test succeeded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Postprocess Server Output\n",
    "\n",
    "**Exercise:** In the serving_input_to_output() function, the last step is postprocess_output(). Modify the output to return the top k classes and probabilities.\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5\n",
    "\n",
    "def postprocess_output(logits, k=TOP_K):\n",
    "    '''Return top k classes and probabilities from class logits.'''\n",
    "    probs = tf.nn.softmax(logits)  # Converts logits to probabilities.\n",
    "    top_k_probs, top_k_classes = ???\n",
    "    return {'classes': top_k_classes, 'probabilities': top_k_probs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test the Output Postprocessing Helper Function\n",
    "\n",
    "**Exercise:** Fill in the shape field for the output logits tensor. \n",
    "\n",
    "**Hint:** how many image classes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Output Postprocessing Network: only needs to be run once!\n",
    "test_logits_tensor = tf.placeholder(dtype=tf.float32, shape=???, name='test_logits_tensor')\n",
    "test_prediction_output = postprocess_output(test_logits_tensor)\n",
    "\n",
    "# Print the graph elements to check shapes.\n",
    "print(test_logits_tensor)\n",
    "print(test_prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test network\n",
    "with tf.Session() as sess:\n",
    "    logits = np.ones(1001)\n",
    "    result = sess.run(test_prediction_output, {test_logits_tensor: logits})\n",
    "    classes = result['classes']\n",
    "    probs = result['probabilities']\n",
    "    # Inefficient but simple element-wise check\n",
    "    assert probs[1:].all() == probs[:-1].all()\n",
    "    # Check that they equal 1 / 1001\n",
    "    expected_probs = np.array(len(probs) * [1.0/1001.0])\n",
    "    assert probs.all() == expected_probs.all()\n",
    "    print('Hooray! Output unit test succeeded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Helper Function\n",
    "\n",
    "We will now integrate the input helper function, output helper function, and network together into a serving_input_to_output() function, which is called by the serving_model_fn() above. Essentially, this function defines an end-to-end graph thta takes an input jpeg tensor, converts it to a 4d floating point tensor, runs the tensor through the ResNet50 network, and postprocesses the output to return a dictionary of the top k predicted classes and probabilities.\n",
    "\n",
    "Normally, we would want to create an integration test for this end-to-end function. However, to avoid replicating the entire ResNet50 network and causing potential memory issues in a notebook environment, we instead provide an example of integration testing in the [Estimator Unit Test](./testing/estimator_unit_test.py) python file.\n",
    "\n",
    "**Exercise:** The function below works as is, but you may want to change the data_format argument below depending on whether you are deploying serving in a CPU only or GPU environment. For convolutional neural nets, it has been shown that placing your color channels ('channels_first') before your pixel dimensions in the image tensor significantly improves performance over 'channels_last'. \n",
    "\n",
    "**Note:** If you are running serving with GPU, feel free to try out both data formats, i.e. 'channels_first' and 'channels_last', to compare performances during serving. HOWEVER, in [the next notebook](./resnet_servable_validation.ipynb) where you will validate the servable model that you produced in this step, 'channels_last' is required due to limitations in the tf.contrib.predict package. If you want to validate your servable, we suggest you start by creating a servable with data format 'channels_last' for validation, then recreate a servable with 'channels_first' as this should also work without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_to_output(features, mode, k=TOP_K):\n",
    "    # Preprocess inputs before sending tensors to the network.\n",
    "    processed_images = preprocess_input(features)\n",
    "\n",
    "    # TODO: Feel free to use 'channels_first' or 'channels_last'\n",
    "    network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES, data_format='channels_last')\n",
    "\n",
    "    # NOTE: No need to change this, but is_training will always be false since we are predicting.\n",
    "    logits = network(\n",
    "      inputs=processed_images, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Postprocess outputs of network (logits) and send top k predictions back to client.\n",
    "    predictions = postprocess_output(logits, k=k)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Servable Model API Definition\n",
    "\n",
    "The last step in serving_model_fn() is to return an EstimatorSpec containing instructions for the Estimator to export a servable model. EstimatorSpec contains a field `export_outputs`, which defines the dictionary of fields that the servable model will return to a client upon receiving a request. To export the predictions dictionary above using Tf serving, you will need to assign the export_outputs parameter in EstimatorSpec.\n",
    "\n",
    "**Exercise:** Add a dictionary with a string key which will be the request.model_spec.signature_name that\n",
    "your client will call in [client/resnet_client.py](./client/resnet_client.py)\n",
    "Add a value that is tf.estimator.export.PredictOutput(outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_servable_estimator_spec(predictions, mode):\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,  # Note: This is not be used in serving, but must be provided for the Estimator API.\n",
    "      ??? # TODO: assign an appropriate dictionary to the export_outputs parameter here.\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Estimator\n",
    "\n",
    "Create an estimator with the serving_model_fn defined above. model_dir specifies where to look for saved model parameters from training, namely, MODEL_DIR where we downloaded and extracted checkpoint files to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=serving_model_fn,\n",
    "  model_dir=MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving input receiver function\n",
    "\n",
    "Finally, exporting the model requires a function that explicitly tells the server what to expect as input from the client, namely, the serving_input_receiver_fn. \n",
    "\n",
    "**Exercise:** Replace the input to [build_raw_serving_input_receiver_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_parsing_serving_input_receiver_fn) below with the expected format of features in the serving_model_fn, i.e. {'images': tf.placeholder(...)}. \n",
    "\n",
    "**Hint:** it's identical to the placeholder in your input unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "  return tf.estimator.export.build_raw_serving_input_receiver_fn(???)()  ## TODO: Add dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the servable model to disk\n",
    "\n",
    "Assuming all of your unit tests have succeeded, and your serving_model_fn() is implemented correctly, this step should successfully export a saved model to disk in the SERVING_DIR specified above. If not, look through the logs to find the point of failure in one of your above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export_savedmodel(export_dir_base=SERVING_DIR,\n",
    "                            serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
