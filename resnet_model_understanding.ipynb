{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Resnet Model Features\n",
    "\n",
    "We know that the Resnet model works well, but why does it work? How can we have confidence that it is searching out the correct features? A recent paper, [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf), shows that averaging gradients taken along a path of images from a blank image (e.g. pure black or grey) to the actual image, can robustly predict sets of pixels that have a strong impact on the overall classification of the image. The below code shows how to modify the TF estimator code to analyze model behavior of different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_IMAGE_SIZE = 224\n",
    "_NUM_CHANNELS = 3\n",
    "_LABEL_CLASSES = 1001\n",
    "\n",
    "_MOMENTUM = 0.9\n",
    "_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5\n",
    "\n",
    "RESNET_SIZE = 50  # We're loading a resnet-50 saved model.\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR='resnet_model_checkpoints'\n",
    "VIS_DIR='visualization'\n",
    "\n",
    "# RIEMANN STEPS is the number of steps in a Riemann Sum.\n",
    "# This is used to compute an approximate the integral of gradients by supplying\n",
    "# images on the path from a blank image to the original image.\n",
    "RIEMANN_STEPS = 50\n",
    "\n",
    "# Return the top k classes and probabilities, so we can also visualize model inference\n",
    "# against other contending classes besides the most likely class.\n",
    "TOP_K = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "\n",
    "The next step is to load the researcher's saved checkpoint into our estimator. We will download it from\n",
    "http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz using the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz \", \"resnet.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the file into a directory called resnet\n",
    "call([\"mkdir\", MODEL_DIR])\n",
    "call([\"tar\", \"-zxvf\", \"resnet.tar.gz\", \"-C\", MODEL_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you see model checkpoint files in this directory\n",
    "os.listdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    " \n",
    " These helper functions define the neural network that was used to train the model. When we load a trained estimator checkpoint, we need to ensure that the neural network definitions are IDENTICAL to that used in training, since the checkpoint will include weights and bias values pertaining to specific neurons in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_relu(inputs, is_training, data_format):\n",
    "  \"\"\"Performs a batch normalization followed by a ReLU.\"\"\"\n",
    "  # We set fused=True for a significant performance boost. See\n",
    "  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n",
    "  inputs = tf.layers.batch_normalization(\n",
    "      inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
    "      momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,\n",
    "      scale=True, training=is_training, fused=True)\n",
    "  inputs = tf.nn.relu(inputs)\n",
    "  return inputs\n",
    "\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "  \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "                 Should be a positive integer.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "  Returns:\n",
    "    A tensor with the same format as the input with the data either intact\n",
    "    (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "  \"\"\"\n",
    "  pad_total = kernel_size - 1\n",
    "  pad_beg = pad_total // 2\n",
    "  pad_end = pad_total - pad_beg\n",
    "\n",
    "  if data_format == 'channels_first':\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
    "                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "  else:\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                    [pad_beg, pad_end], [0, 0]])\n",
    "  return padded_inputs\n",
    "\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n",
    "  \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "  # The padding is consistent and is based only on `kernel_size`, not on the\n",
    "  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n",
    "  if strides > 1:\n",
    "    inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "  return tf.layers.conv2d(\n",
    "      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "      padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n",
    "      kernel_initializer=tf.variance_scaling_initializer(),\n",
    "      data_format=data_format)\n",
    "\n",
    "\n",
    "def building_block(inputs, filters, is_training, projection_shortcut, strides,\n",
    "                   data_format):\n",
    "  \"\"\"Standard building block for residual networks with BN before convolutions.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the convolutions.\n",
    "    is_training: A Boolean for whether the model is in training or inference\n",
    "      mode. Needed for batch normalization.\n",
    "    projection_shortcut: The function to use for projection shortcuts (typically\n",
    "      a 1x1 convolution when downsampling the input).\n",
    "    strides: The block's stride. If greater than 1, this block will ultimately\n",
    "      downsample the input.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "  Returns:\n",
    "    The output tensor of the block.\n",
    "  \"\"\"\n",
    "  shortcut = inputs\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "\n",
    "  # The projection shortcut should come after the first batch norm and ReLU\n",
    "  # since it performs a 1x1 convolution.\n",
    "  if projection_shortcut is not None:\n",
    "    shortcut = projection_shortcut(inputs)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  return inputs + shortcut\n",
    "\n",
    "\n",
    "def bottleneck_block(inputs, filters, is_training, projection_shortcut,\n",
    "                     strides, data_format):\n",
    "  \"\"\"Bottleneck block variant for residual networks with BN before convolutions.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the first two convolutions. Note that the\n",
    "      third and final convolution will use 4 times as many filters.\n",
    "    is_training: A Boolean for whether the model is in training or inference\n",
    "      mode. Needed for batch normalization.\n",
    "    projection_shortcut: The function to use for projection shortcuts (typically\n",
    "      a 1x1 convolution when downsampling the input).\n",
    "    strides: The block's stride. If greater than 1, this block will ultimately\n",
    "      downsample the input.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "  Returns:\n",
    "    The output tensor of the block.\n",
    "  \"\"\"\n",
    "  shortcut = inputs\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "\n",
    "  # The projection shortcut should come after the first batch norm and ReLU\n",
    "  # since it performs a 1x1 convolution.\n",
    "  if projection_shortcut is not None:\n",
    "    shortcut = projection_shortcut(inputs)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=1, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
    "      data_format=data_format)\n",
    "\n",
    "  inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=4 * filters, kernel_size=1, strides=1,\n",
    "      data_format=data_format)\n",
    "\n",
    "  return inputs + shortcut\n",
    "\n",
    "\n",
    "def block_layer(inputs, filters, block_fn, blocks, strides, is_training, name,\n",
    "                data_format):\n",
    "  \"\"\"Creates one layer of blocks for the ResNet model.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the first convolution of the layer.\n",
    "    block_fn: The block to use within the model, either `building_block` or\n",
    "      `bottleneck_block`.\n",
    "    blocks: The number of blocks contained in the layer.\n",
    "    strides: The stride to use for the first convolution of the layer. If\n",
    "      greater than 1, this layer will ultimately downsample the input.\n",
    "    is_training: Either True or False, whether we are currently training the\n",
    "      model. Needed for batch norm.\n",
    "    name: A string name for the tensor output of the block layer.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "  Returns:\n",
    "    The output tensor of the block layer.\n",
    "  \"\"\"\n",
    "  # Bottleneck blocks end with 4x the number of filters as they start with\n",
    "  filters_out = 4 * filters if block_fn is bottleneck_block else filters\n",
    "\n",
    "  def projection_shortcut(inputs):\n",
    "    return conv2d_fixed_padding(\n",
    "        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "\n",
    "  # Only the first block per block_layer uses projection_shortcut and strides\n",
    "  inputs = block_fn(inputs, filters, is_training, projection_shortcut, strides,\n",
    "                    data_format)\n",
    "\n",
    "  for _ in range(1, blocks):\n",
    "    inputs = block_fn(inputs, filters, is_training, None, 1, data_format)\n",
    "\n",
    "  return tf.identity(inputs, name)\n",
    "\n",
    "\n",
    "def imagenet_resnet_v2_generator(block_fn, layers, num_classes,\n",
    "                                 data_format=None):\n",
    "  \"\"\"Generator for ImageNet ResNet v2 models.\n",
    "  Args:\n",
    "    block_fn: The block to use within the model, either `building_block` or\n",
    "      `bottleneck_block`.\n",
    "    layers: A length-4 array denoting the number of blocks to include in each\n",
    "      layer. Each layer consists of blocks that take inputs of the same size.\n",
    "    num_classes: The number of possible classes for image classification.\n",
    "    data_format: The input format ('channels_last', 'channels_first', or None).\n",
    "      If set to None, the format is dependent on whether a GPU is available.\n",
    "  Returns:\n",
    "    The model function that takes in `inputs` and `is_training` and\n",
    "    returns the output tensor of the ResNet model.\n",
    "  \"\"\"\n",
    "  if data_format is None:\n",
    "    data_format = (\n",
    "        'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "\n",
    "  def model(inputs, is_training):\n",
    "    \"\"\"Constructs the ResNet model given the inputs.\"\"\"\n",
    "    if data_format == 'channels_first':\n",
    "      # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n",
    "      # This provides a large performance boost on GPU. See\n",
    "      # https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "      inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs=inputs, filters=64, kernel_size=7, strides=2,\n",
    "        data_format=data_format)\n",
    "    inputs = tf.identity(inputs, 'initial_conv')\n",
    "    inputs = tf.layers.max_pooling2d(\n",
    "        inputs=inputs, pool_size=3, strides=2, padding='SAME',\n",
    "        data_format=data_format)\n",
    "    inputs = tf.identity(inputs, 'initial_max_pool')\n",
    "\n",
    "    inputs = block_layer(\n",
    "        inputs=inputs, filters=64, block_fn=block_fn, blocks=layers[0],\n",
    "        strides=1, is_training=is_training, name='block_layer1',\n",
    "        data_format=data_format)\n",
    "    inputs = block_layer(\n",
    "        inputs=inputs, filters=128, block_fn=block_fn, blocks=layers[1],\n",
    "        strides=2, is_training=is_training, name='block_layer2',\n",
    "        data_format=data_format)\n",
    "    inputs = block_layer(\n",
    "        inputs=inputs, filters=256, block_fn=block_fn, blocks=layers[2],\n",
    "        strides=2, is_training=is_training, name='block_layer3',\n",
    "        data_format=data_format)\n",
    "    inputs = block_layer(\n",
    "        inputs=inputs, filters=512, block_fn=block_fn, blocks=layers[3],\n",
    "        strides=2, is_training=is_training, name='block_layer4',\n",
    "        data_format=data_format)\n",
    "\n",
    "    inputs = batch_norm_relu(inputs, is_training, data_format)\n",
    "    inputs = tf.layers.average_pooling2d(\n",
    "        inputs=inputs, pool_size=7, strides=1, padding='VALID',\n",
    "        data_format=data_format)\n",
    "    inputs = tf.identity(inputs, 'final_avg_pool')\n",
    "    inputs = tf.reshape(inputs,\n",
    "                        [-1, 512 if block_fn is building_block else 2048])\n",
    "    inputs = tf.layers.dense(inputs=inputs, units=num_classes)\n",
    "    inputs = tf.identity(inputs, 'final_dense')\n",
    "    return inputs\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def imagenet_resnet_v2(resnet_size, num_classes, data_format=None):\n",
    "  \"\"\"Returns the ResNet model for a given size and number of output classes.\"\"\"\n",
    "  model_params = {\n",
    "      18: {'block': building_block, 'layers': [2, 2, 2, 2]},\n",
    "      34: {'block': building_block, 'layers': [3, 4, 6, 3]},\n",
    "      50: {'block': bottleneck_block, 'layers': [3, 4, 6, 3]},\n",
    "      101: {'block': bottleneck_block, 'layers': [3, 4, 23, 3]},\n",
    "      152: {'block': bottleneck_block, 'layers': [3, 8, 36, 3]},\n",
    "      200: {'block': bottleneck_block, 'layers': [3, 24, 36, 3]}\n",
    "  }\n",
    "\n",
    "  if resnet_size not in model_params:\n",
    "    raise ValueError('Not a valid resnet_size:', resnet_size)\n",
    "\n",
    "  params = model_params[resnet_size]\n",
    "  return imagenet_resnet_v2_generator(\n",
    "      params['block'], params['layers'], num_classes, data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing functions\n",
    "\n",
    "Note that preprocessing functions are called during training as well (see https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py and https://github.com/tensorflow/models/blob/master/official/resnet/vgg_preprocessing.py), so we will need to extract relevant logic from these functions. Below is a simplified preprocessing code that normalizes the image's pixel values.\n",
    "\n",
    "For simplicity, we assume the client provides properly-sized images 224 x 224 x 3 in batches. It will become clear later that sending images over ip in protobuf format can be more easily handled by storing a 4d tensor. The only preprocessing required here is to subtract the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "  \"\"\"Preprocesses the image by subtracting out the mean from all channels.\n",
    "  Args:\n",
    "    image: A 4D `Tensor` representing a batch of images.\n",
    "  Returns:\n",
    "    image pixels normalized to be between -0.5 and 0.5\n",
    "  \"\"\"\n",
    "  return tf.to_float(images) / 255 - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet Model Functions\n",
    "\n",
    "We are going to create two estimators here since we need to run two model predictions. \n",
    "\n",
    "* The first prediction computes the top labels for the image by returning the argmax_k top logits. \n",
    "\n",
    "* The second prediction returns a sequence of gradients along the straightline path from a purely grey image (127.5, 127.5, 127.5) to the final image. We use grey here because the resnet model transforms this pixel value to all 0s.\n",
    "\n",
    "Below is the resnet model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model_fn(features, labels, mode):\n",
    "  \"\"\"Our model_fn for ResNet to be used with our Estimator.\"\"\"\n",
    "\n",
    "  # Preprocess images as necessary for resnet\n",
    "  features = preprocess_images(features['images'])\n",
    "\n",
    "  # This network must be IDENTICAL to that used to train.\n",
    "  network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES)\n",
    "\n",
    "  # tf.estimator.ModeKeys.TRAIN will be false since we are predicting.\n",
    "  logits = network(\n",
    "      inputs=features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "  # Instead of the top 1 result, we can now return top k!\n",
    "  top_k_logits, top_k_classes = tf.nn.top_k(logits, k=TOP_K)\n",
    "  top_k_probs = tf.nn.softmax(top_k_logits)\n",
    "  predictions = {\n",
    "      'classes': top_k_classes,\n",
    "      'probabilities': top_k_probs\n",
    "  }\n",
    "\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions, \n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients Model Function\n",
    "\n",
    "The Gradients model function takes as input a single image (a 4d tensor of dimension [1, 244, 244, 3]) and expands it to a series of images (tensor dimension [RIEMANN_STEPS + 1, 244, 244, 3]), where each image is simply a \"fractional\" image, with image 0 being pure gray to image RIEMANN_STEPS being the original image. The gradients are then computed for each of these images, and various outputs are returned.\n",
    "\n",
    "**Note:** Each step is a single inference that returns an entire gradient pixel map.\n",
    "The total gradient map evaluation can take a couple minutes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_model_fn(features, labels, mode):\n",
    "  \"\"\"Our model_fn for ResNet to be used with our Estimator.\"\"\"\n",
    "    \n",
    "  # Supply labels from features dict since prediction mode causes labels to be none\n",
    "  labels = features['labels']\n",
    "    \n",
    "  # Features here is a 4d tensor of ONE image. Normalize it as in training and serving.\n",
    "  features = preprocess_images(features['images'])\n",
    "\n",
    "  # This network must be IDENTICAL to that used to train.\n",
    "  network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES)\n",
    "\n",
    "  # path_features should have dim [RIEMANN_STEPS + 1, 224, 224, 3]\n",
    "  path_features = tf.zeros([1, 224, 224, 3])\n",
    "  for i in range(1, RIEMANN_STEPS + 1):\n",
    "    path_features = tf.concat([path_features, features * i / RIEMANN_STEPS], axis=0)\n",
    "   \n",
    "  # Path logits should evaluate logits for each path feature and return a 2d array for all path images and classes\n",
    "  path_logits = network(inputs=path_features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "  # The logit we care about is only that pertaining to the label. Labels has only one element, so retrieve it.\n",
    "  target_logits = path_logits[:, labels[0]]\n",
    "   \n",
    "  # Compute gradients for each image with respect to each logit\n",
    "  gradients = tf.gradients(target_logits, path_features)\n",
    "    \n",
    "  # Multiply elementwise to the original image to get weighted gradients for each pixel.\n",
    "  gradients = tf.squeeze(tf.multiply(gradients, features))\n",
    "    \n",
    "  predictions = {\n",
    "      'path_features': path_features,  # for debugging\n",
    "      'path_logits': path_logits,  # for debugging\n",
    "      'target_logits': target_logits,  # use this to verify that the riemann integral works out\n",
    "      'path_features': path_features, # for displaying path images\n",
    "      'gradients': gradients  # for displaying gradient images and computing integrated gradient\n",
    "  }\n",
    "\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions,  # This is the returned value\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "Load in the model_fn using the checkpoints from MODEL_DIR. This will initialize our weights which we will then use to run backpropagation to find integrated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this model into our estimator\n",
    "resnet_estimator = tf.estimator.Estimator(\n",
    "  model_fn=resnet_model_fn,  # Call our generate_model_fn to create model function\n",
    "  model_dir=MODEL_DIR,  # Where to look for model checkpoints\n",
    "  #config not needed\n",
    ")\n",
    "\n",
    "gradients_estimator = tf.estimator.Estimator(\n",
    "  model_fn=gradients_model_fn,  # Call our generate_model_fn to create model function\n",
    "  model_dir=MODEL_DIR,  # Where to look for model checkpoints\n",
    "  #config not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create properly sized image in numpy\n",
    "\n",
    "Load whatever image you would like (local or url), and resize to 224 x 224 x 3 using opencv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(img, output_image_dim=_DEFAULT_IMAGE_SIZE):\n",
    "  \"\"\"Resize the image to make it IMAGE_DIM x IMAGE_DIM pixels in size.\n",
    "\n",
    "  If an image is not square, it will pad the top/bottom or left/right\n",
    "  with black pixels to ensure the image is square.\n",
    "\n",
    "  Args:\n",
    "    img: the input 3-color image\n",
    "    output_image_dim: resized and padded output length (and width)\n",
    "\n",
    "  Returns:\n",
    "    resized and padded image\n",
    "  \"\"\"\n",
    "\n",
    "  h, w = img.shape[:2]\n",
    "\n",
    "  # interpolation method\n",
    "  if h > output_image_dim or w > output_image_dim:\n",
    "    # use preferred interpolation method for shrinking image\n",
    "    interp = cv2.INTER_AREA\n",
    "  else:\n",
    "    # use preferred interpolation method for stretching image\n",
    "    interp = cv2.INTER_CUBIC\n",
    "\n",
    "  # aspect ratio of image\n",
    "  aspect = float(w) / h\n",
    "\n",
    "  # compute scaling and pad sizing\n",
    "  if aspect > 1:  # Image is \"wide\". Add black pixels on top and bottom.\n",
    "    new_w = output_image_dim\n",
    "    new_h = np.round(new_w / aspect)\n",
    "    pad_vert = (output_image_dim - new_h) / 2\n",
    "    pad_top, pad_bot = int(np.floor(pad_vert)), int(np.ceil(pad_vert))\n",
    "    pad_left, pad_right = 0, 0\n",
    "  elif aspect < 1:  # Image is \"tall\". Add black pixels on left and right.\n",
    "    new_h = output_image_dim\n",
    "    new_w = np.round(new_h * aspect)\n",
    "    pad_horz = (output_image_dim - new_w) / 2\n",
    "    pad_left, pad_right = int(np.floor(pad_horz)), int(np.ceil(pad_horz))\n",
    "    pad_top, pad_bot = 0, 0\n",
    "  else:  # square image\n",
    "    new_h = output_image_dim\n",
    "    new_w = output_image_dim\n",
    "    pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "  # scale to IMAGE_DIM x IMAGE_DIM and pad with zeros (black pixels)\n",
    "  scaled_img = cv2.resize(img, (int(new_w), int(new_h)), interpolation=interp)\n",
    "  scaled_img = cv2.copyMakeBorder(scaled_img,\n",
    "                                  pad_top, pad_bot, pad_left, pad_right,\n",
    "                                  borderType=cv2.BORDER_CONSTANT, value=[127, 127, 127]) # Grey border\n",
    "  return scaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = 'http://homecookingadventure.com/images/recipes/Chocolate_Mirror_Cake_main.jpg'\n",
    "IMAGE_NAME = os.path.splitext(os.path.basename(IMAGE))[0]\n",
    "print(IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = None\n",
    "if 'http' in IMAGE:\n",
    "  resp = urllib.request.urlopen(IMAGE)\n",
    "  feature = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "  feature = cv2.imdecode(feature, cv2.IMREAD_COLOR)\n",
    "else:\n",
    "  feature = cv2.imread(IMAGE)  # Parse the image from your local disk.\n",
    "\n",
    "feature = cv2.cvtColor(feature, cv2.COLOR_BGR2RGB)  # Flip the RGB (cv2 issue)\n",
    "# Resize and pad the image\n",
    "feature = resize_and_pad_image(feature)\n",
    "# Append to features_array\n",
    "feature = np.array([feature], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image to validate\n",
    "imgplot = plt.imshow(feature[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Input Function\n",
    "\n",
    "Since we are analyzing the model using the estimator api, we need to provide an input function for prediction. Fortunately, there are built-in input functions that can read from numpy arrays, e.g. tf.estimator.inputs.numpy_input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictions = resnet_estimator.predict(\n",
    "    tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': feature},\n",
    "        shuffle=False\n",
    "    )\n",
    ")\n",
    "\n",
    "label_dict = next(label_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out probabilities and class names\n",
    "classval = label_dict['classes']\n",
    "probsval = label_dict['probabilities']\n",
    "labels = []\n",
    "with open('imagenet1000_clsid_to_human.txt', 'r') as f:\n",
    "  label_reader = csv.reader(f, delimiter=':', quotechar='\\'')\n",
    "  for row in label_reader:\n",
    "    labels.append(row[1][:-1])\n",
    "# The served model uses 0 as the miscellaneous class, and so starts indexing\n",
    "# the imagenet images from 1. Subtract 1 to reference the text correctly.\n",
    "classval = [labels[x - 1] for x in classval]\n",
    "class_and_probs = [str(p) + ' : ' + c for c, p in zip(classval, probsval)]\n",
    "for j in range(0, 5):\n",
    "  print(class_and_probs[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradients\n",
    "\n",
    "Run the gradients estimator to retrieve a generator of metrics and gradient pictures, and pickle the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the visualization directory\n",
    "IMAGE_DIR = os.path.join(VIS_DIR, IMAGE_NAME)\n",
    "call(['mkdir', '-p', IMAGE_DIR])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the top classes. 0 picks out the best, 1 picks out second best, etc...\n",
    "best_label = label_dict['classes'][0]\n",
    "\n",
    "# Compute gradients with respect to this class\n",
    "gradient_predictions = gradients_estimator.predict(\n",
    "    tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': feature, 'labels': np.array([best_label])},\n",
    "        shuffle=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start computing the sum of gradients (to be used for integrated gradients)\n",
    "int_gradients = np.zeros((224, 224, 3))\n",
    "gradients_and_logits = []\n",
    "\n",
    "# Print gradients along the path, and pickle them\n",
    "for i in range(0, RIEMANN_STEPS + 1):\n",
    "    gradient_dict = next(gradient_predictions)\n",
    "    gradient_map = gradient_dict['gradients']\n",
    "    print('Path image %d: gradient: %f, logit: %f' % (i, np.sum(gradient_map), gradient_dict['target_logits']))\n",
    "    # Gradient visualization output pickles\n",
    "    pickle.dump(gradient_map, open(os.path.join(IMAGE_DIR, 'path_gradient_' + str(i) + '.pkl'), \"wb\" ))\n",
    "    int_gradients = np.add(int_gradients, gradient_map)\n",
    "    gradients_and_logits.append((np.sum(gradient_map), gradient_dict['target_logits']))\n",
    "    \n",
    "pickle.dump(int_gradients, open(os.path.join(IMAGE_DIR, 'int_gradients.pkl'), \"wb\" ))\n",
    "pickle.dump(gradients_and_logits, open(os.path.join(IMAGE_DIR, 'gradients_and_logits.pkl'), \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "If you simply want to play around with visualization, unpickle the result from above so you do not have to rerun prediction again. The following visualizes the gradients with different amplification of pixels, and prints their derivatives and logits as well to view where the biggest differentiators lie. You can also modify the INTERPOLATION flag to increase the \"fatness\" of pixels.\n",
    "\n",
    "Below are two examples of visualization methods: one computing the gradient value normalized to between 0 and 1, and another visualizing absolute deviation from the median.\n",
    "\n",
    "## Plotting individual image gradients along path\n",
    "\n",
    "First, let us plot the individual gradient value for all gradient path images. Pay special attention to the images with a large positive gradient (i.e. in the direction of increasing logit for the most likely class). Do the pixel gradients resemble the image class you are trying to detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "gradients_and_logits = pickle.load(open(os.path.join(IMAGE_DIR, 'gradients_and_logits.pkl'), \"rb\" ))\n",
    "for i in range(0, RIEMANN_STEPS + 1):\n",
    "    gradient_map = pickle.load(open(os.path.join(IMAGE_DIR, 'path_gradient_' + str(i) + '.pkl'), \"rb\" ))\n",
    "    min_grad = np.ndarray.min(gradient_map)\n",
    "    max_grad = np.ndarray.max(gradient_map)\n",
    "    median_grad = np.median(gradient_map)\n",
    "    gradient_and_logit = gradients_and_logits[i]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Image %d: grad: %.2f, logit: %.2f' % (i, gradient_and_logit[0], gradient_and_logit[1]))\n",
    "    imgplot = plt.imshow((gradient_map - min_grad) / (max_grad - min_grad),\n",
    "                        interpolation=INTERPOLATION)\n",
    "    plt.subplot(122)\n",
    "    plt.title('Image %d: grad: %.2f, logit: %.2f' % (i, gradient_and_logit[0], gradient_and_logit[1]))\n",
    "    imgplot = plt.imshow(np.abs(gradient_map - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                       interpolation=INTERPOLATION)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Integrated Gradient\n",
    "\n",
    "When integrating over all gradients along the path, the result is an image that captures larger signals from pixels with the large gradients. Is the integrated gradient a clear representation of what it is trying to detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "# Plot the integrated gradients\n",
    "int_gradients = pickle.load(open(os.path.join(IMAGE_DIR, 'int_gradients.pkl'), \"rb\" ))\n",
    "min_grad = np.ndarray.min(int_gradients)\n",
    "max_grad = np.ndarray.max(int_gradients)\n",
    "median_grad = np.median(int_gradients)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(131)\n",
    "imgplot = plt.imshow((int_gradients - min_grad) / (max_grad - min_grad),\n",
    "                    interpolation=INTERPOLATION)\n",
    "plt.subplot(132)\n",
    "imgplot = plt.imshow(np.abs(int_gradients - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                        interpolation=INTERPOLATION)\n",
    "plt.subplot(133)\n",
    "imgplot = plt.imshow(feature[0])\n",
    "plt.show()\n",
    "\n",
    "# Verify that the average of gradients is equal to the difference in logits\n",
    "print('total logit diff: %f' % (gradients_and_logits[RIEMANN_STEPS][1] - gradients_and_logits[0][1]))\n",
    "print('sum of integrated gradients: %f' % (np.sum(int_gradients) / RIEMANN_STEPS + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the integrated gradients for each channel\n",
    "\n",
    "We can also visualize individual pixel contributions from different RGB channels.\n",
    "\n",
    "Can you think of any other visualization ideas to try out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "# Show red-green-blue channels for integrated gradients\n",
    "for channel in range(0, 3):\n",
    "    gradient_channel = int_gradients[:,:,channel]\n",
    "    min_grad = np.ndarray.min(gradient_channel)\n",
    "    max_grad = np.ndarray.max(gradient_channel)\n",
    "    median_grad = np.median(gradient_channel)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    imgplot = plt.imshow((gradient_channel - min_grad) / (max_grad - min_grad),\n",
    "                         interpolation=INTERPOLATION,\n",
    "                         cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    imgplot = plt.imshow(np.abs(gradient_channel - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                         interpolation=INTERPOLATION,\n",
    "                         cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
