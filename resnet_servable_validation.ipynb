{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Resnet Servable\n",
    "\n",
    "Use this notebook to validate the servable that you just created, whether using Estimator or Keras!\n",
    "This notebook can be used to validate that the servable you just trained works as intended by mocking part of the resnet client and feeding the input to the network.\n",
    "\n",
    "**NOTE:** If you're building the ResNet servable from the Estimator API, the below validation only works if you trained ResNet with 'channels_last'. Not to fear! If the servable is validated using 'channels_last' mode, it will work with 'channels_first' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import predictor\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predict function from your saved model (servable)\n",
    "# TODO: Use your own exported resnet serving directory and version number from the output of the last cell in\n",
    "# `resnet_training_to_serving.ipynb` or `keras_resnet_serving.ipynb`.\n",
    "SERVING_DIR = ???\n",
    "VERSION = ???\n",
    "\n",
    "predict_fn = predictor.from_saved_model(os.path.join(SERVING_DIR, str(VERSION)),\n",
    "                                        signature_def_key='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5  # Set this depending on how many classes the server is expected to return\n",
    "\n",
    "# Load a sample jpeg, and send it to the predict function (TF serving emulator)\n",
    "IMAGE_PATH='client/cat_sample.jpg'\n",
    "\n",
    "feature = cv2.imread(IMAGE_PATH)\n",
    "feature = cv2.imencode('.jpg', feature)[1].tostring()\n",
    "predictions = predict_fn(\n",
    "    {'images': [feature]})\n",
    "\n",
    "# Run some test cases\n",
    "classes = predictions['classes']\n",
    "probabilities = predictions['probabilities']\n",
    "assert len(classes[0]) == TOP_K\n",
    "assert len(probabilities[0]) == TOP_K\n",
    "assert max(probabilities[0]) <= 1\n",
    "assert max(probabilities[0]) >= 1.0 / TOP_K\n",
    "assert min(probabilities[0]) >= 0\n",
    "# Print your model output for inspection\n",
    "print('Model passes unit test! Here are some predictions:\\n')\n",
    "print(predictions)\n",
    "\n",
    "# If this succeeds, congratulations! Your model (probably) works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
